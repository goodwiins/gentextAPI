{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-whole",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip3 install benepar\n",
    "!pip3 install summa\n",
    "!pip3 install nltk\n",
    "!pip3 install spacy\n",
    "!python3 -m spacy download en\n",
    "!pip3 install scipy\n",
    "!pip3 install sentence-transformers\n",
    "!pip3 install transformers\n",
    "!pip3 install tensorflow\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confirmed-authorization",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:18:09.617829Z",
     "start_time": "2024-03-12T20:18:09.610549Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from summa.summarizer import summarize\n",
    "import benepar\n",
    "import string\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import torch\n",
    "import summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "banner-leonard",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:18:22.267509Z",
     "start_time": "2024-03-12T20:18:13.448539Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (3.0.9)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: benepar in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: nltk>=3.2 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from benepar) (3.8.1)\r\n",
      "Requirement already satisfied: spacy>=2.0.9 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from benepar) (3.7.4)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from benepar) (2.2.1)\r\n",
      "Requirement already satisfied: torch-struct>=0.5 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from benepar) (0.5)\r\n",
      "Requirement already satisfied: tokenizers>=0.9.4 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from benepar) (0.15.2)\r\n",
      "Collecting transformers>=4.2.2 (from transformers[tokenizers,torch]>=4.2.2->benepar)\r\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\r\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from benepar) (4.25.3)\r\n",
      "Requirement already satisfied: sentencepiece>=0.1.91 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from benepar) (0.2.0)\r\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from nltk>=3.2->benepar) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from nltk>=3.2->benepar) (1.3.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from nltk>=3.2->benepar) (2023.12.25)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from nltk>=3.2->benepar) (4.66.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (8.2.3)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (6.4.0)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (2.6.4)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (3.1.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (68.2.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (23.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from spacy>=2.0.9->benepar) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from tokenizers>=0.9.4->benepar) (0.21.4)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from torch>=1.6.0->benepar) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from torch>=1.6.0->benepar) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from torch>=1.6.0->benepar) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from torch>=1.6.0->benepar) (3.2.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from torch>=1.6.0->benepar) (2024.2.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.4.2)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.28.0)\r\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from accelerate>=0.21.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (2.16.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.0.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2024.2.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.9->benepar) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.9->benepar) (0.1.4)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.0.9->benepar) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from jinja2->spacy>=2.0.9->benepar) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages (from sympy->torch>=1.6.0->benepar) (1.3.0)\r\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 2.3.0\r\n",
      "    Uninstalling transformers-2.3.0:\r\n",
      "      Successfully uninstalled transformers-2.3.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "sentence-transformers 0.2.5.1 requires transformers==2.3.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed transformers-4.38.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install cython\n",
    "!pip3 install benepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7afaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:12:01.017096Z",
     "start_time": "2024-03-12T20:12:00.846334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: unknown file attribute: b\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!benepar.download('benepar_en3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "confidential-handy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:18:39.498961Z",
     "start_time": "2024-03-12T20:18:24.091239Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "# nltk.download('punkt')\n",
    "# benepar.download()\n",
    "benepar_parser = benepar.Parser(\"benepar_en3\")\n",
    "import summa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "equivalent-assignment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:18:54.346051Z",
     "start_time": "2024-03-12T20:18:54.333590Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"text.txt\"\n",
    "def read_file(file_path):\n",
    "  with open(file_path,'r') as content_file:\n",
    "    content = content_file.read()\n",
    "    return content\n",
    "text = read_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "remarkable-chicago",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:19:00.441362Z",
     "start_time": "2024-03-12T20:19:00.306594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If a local government tax is legally dedicated for one or more specific purposes it is a special tax\n",
      "\n",
      "\n",
      "53] Proposition 218 also requires certain taxes relating to real property (e.g., parcel taxes) be levied as special taxes\n",
      "\n",
      "\n",
      "Section 2 of Article XIII C contains the voter approval requirements for local government taxes\n",
      "\n",
      "\n",
      "Under Proposition 218, every local government tax is either a general tax or a special tax\n",
      "\n",
      "\n",
      "55] Proposition 218 does not allow a local tax to be a hybrid tax\n",
      "\n",
      "\n",
      "62] The applicable electorate for conducting a local tax election is generally the registered voters of the local government\n",
      "\n",
      "\n",
      "63][64]\n",
      "New local government taxes require voter approval under Proposition 218\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def preprocess(sentences):\n",
    "  output = []\n",
    "  for sent in sentences:\n",
    "    single_quotes_present = len(re.findall(r\"['][\\w\\s.:;,!?\\\\-]+[']\",sent))>0\n",
    "    double_quotes_present = len(re.findall(r'[\"][\\w\\s.:;,!?\\\\-]+[\"]',sent))>0\n",
    "    question_present = \"?\" in sent\n",
    "    if single_quotes_present or double_quotes_present or question_present:\n",
    "      continue\n",
    "    else:\n",
    "      output.append(sent.strip(punctuation))\n",
    "  return output\n",
    "\n",
    "\n",
    "def get_candidate_sents(r_text,ratio = .3):\n",
    "  candidate_sents = summarize(r_text,ratio)\n",
    "  candidate_sents_list = tokenize.sent_tokenize(candidate_sents)\n",
    "  candidate_sents_list = [re.split(r'[:;]+',x)[0] for x in candidate_sents_list ]\n",
    "  filtered_list_short_sentences = [sent for sent in candidate_sents_list if len(sent)>30 and len(sent)<150]\n",
    "  return filtered_list_short_sentences\n",
    "\n",
    "\n",
    "cand_sent = get_candidate_sents(text)\n",
    "filter_quotes_and_questions = preprocess(cand_sent)\n",
    "for each_sent in filter_quotes_and_questions:\n",
    "  print(each_sent)\n",
    "  print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "def get_flattend(t):\n",
    "  sent_str_final = None\n",
    "  if t is not None:\n",
    "    sent_str = [\" \".join(x.leaves()) for x in list(t)]\n",
    "    sent_str_final = [\" \".join(sent_str)]\n",
    "    sent_str_final = sent_str_final[0]\n",
    "    return sent_str_final\n",
    "\n",
    "def get_termination_portion(main_string, sub_string):\n",
    "    combined_sub_string = sub_string.replace(\" \",\"\")\n",
    "    main_string_list = main_string.split()\n",
    "    last_index = len(main_string_list)\n",
    "    for i in range(last_index):\n",
    "        check_string_list = main_string_list[i:]\n",
    "        check_string = \"\".join(check_string_list)\n",
    "        check_string = check_string.replace(\" \",\"\")\n",
    "        if check_string == combined_sub_string:\n",
    "            return \" \".join(main_string_list[:i])\n",
    "    return None\n",
    "\n",
    "def get_right_most_VP_or_NP(parse_tree, last_NP = None, last_VP = None):\n",
    "  if len(parse_tree.leaves()) == 1:\n",
    "    return get_flattend(last_NP) , get_flattend(last_VP)\n",
    "  last_subtree = parse_tree[-1]\n",
    "  if last_subtree.label() == \"NP\":\n",
    "    last_NP = last_subtree\n",
    "  elif last_subtree.label() == \"VP\":\n",
    "    last_vp = last_subtree\n",
    "  return get_right_most_VP_or_NP(last_subtree,last_NP,last_VP)\n",
    "\n",
    "def get_sentence_completions(key_sentences):\n",
    "    sentence_completion_dict = {}\n",
    "    for individual_sentence in filter_quotes_and_questions:\n",
    "        sentence = individual_sentence.rstrip('?:!.,;')\n",
    "        tree = benepar_parser.parse(sentence)\n",
    "        last_nounphrase, last_verbphrase =  get_right_most_VP_or_NP(tree)\n",
    "        phrases= []\n",
    "        if last_verbphrase is not None:\n",
    "            verbphrase_string = get_termination_portion(sentence,last_verbphrase)\n",
    "            phrases.append(verbphrase_string)\n",
    "        if last_nounphrase is not None:\n",
    "            nounphrase_string = get_termination_portion(sentence,last_nounphrase)\n",
    "            phrases.append(nounphrase_string)\n",
    "\n",
    "        longest_phrase =  sorted(phrases, key=len,reverse= True)\n",
    "        if len(longest_phrase) == 2:\n",
    "            first_sent_len = len(longest_phrase[0].split())\n",
    "            second_sentence_len = len(longest_phrase[1].split())\n",
    "            if (first_sent_len - second_sentence_len) > 4:\n",
    "                del longest_phrase[1]\n",
    "                \n",
    "        if len(longest_phrase)>0:\n",
    "            sentence_completion_dict[sentence]=longest_phrase\n",
    "    return sentence_completion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "developing-cambodia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:26:03.201490Z",
     "start_time": "2024-03-12T20:19:01.507224Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NewEnv/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'If a local government tax is legally dedicated for one or more specific purposes it is a special tax': ['If a local government tax is legally dedicated for one or more specific purposes it is'], '53] Proposition 218 also requires certain taxes relating to real property (e.g., parcel taxes) be levied as special taxes': ['53] Proposition 218 also requires certain taxes relating to real property (e.g., parcel taxes) be levied as'], 'Section 2 of Article XIII C contains the voter approval requirements for local government taxes': ['Section 2 of Article XIII C contains the voter approval requirements for'], 'Under Proposition 218, every local government tax is either a general tax or a special tax': ['Under Proposition 218, every local government tax is either a general tax or'], '55] Proposition 218 does not allow a local tax to be a hybrid tax': ['55] Proposition 218 does not allow a local tax to be'], '62] The applicable electorate for conducting a local tax election is generally the registered voters of the local government': ['62] The applicable electorate for conducting a local tax election is generally the registered voters of'], '63][64]\\nNew local government taxes require voter approval under Proposition 218': ['63][64] New local government taxes require voter approval under']}\n"
     ]
    }
   ],
   "source": [
    "sent_completion_dict = get_sentence_completions(filter_quotes_and_questions)\n",
    "print(sent_completion_dict)\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_BERT = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "from nltk import tokenize\n",
    "import scipy\n",
    "torch.manual_seed(2020)\n",
    "\n",
    "\n",
    "def sort_by_similarity(original_sentence,generated_sentences_list):\n",
    "    # Each sentence is encoded as a 1-D vector with 768 columns\n",
    "    sentence_embeddings = model_BERT.encode(generated_sentences_list)\n",
    "\n",
    "    queries = [original_sentence]\n",
    "    query_embeddings = model_BERT.encode(queries)\n",
    "    # Find the top sentences of the corpus for each query sentence based on cosine similarity\n",
    "    number_top_matches = len(generated_sentences_list)\n",
    "\n",
    "    dissimilar_sentences = []\n",
    "\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
    "\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "        for idx, distance in reversed(results[0:number_top_matches]):\n",
    "            score = 1-distance\n",
    "            if score < 0.9:\n",
    "                dissimilar_sentences.append(generated_sentences_list[idx].strip())\n",
    "           \n",
    "    sorted_dissimilar_sentences = sorted(dissimilar_sentences, key=len)\n",
    "    \n",
    "    return sorted_dissimilar_sentences[:3]\n",
    "    \n",
    "\n",
    "def generate_sentences(partial_sentence,full_sentence):\n",
    "    input_ids = torch.tensor([tokenizer.encode(partial_sentence)])\n",
    "    maximum_length = len(partial_sentence.split())+80\n",
    "\n",
    "    # Actiavte top_k sampling and top_p sampling with only from 90% most likely words\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=maximum_length, \n",
    "        top_p=0.90, # 0.85 \n",
    "        top_k=50,   #0.30\n",
    "        repetition_penalty  = 10.0,\n",
    "        num_return_sequences=10\n",
    "    )\n",
    "    generated_sentences=[]\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        decoded_sentences = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "        decoded_sentences_list = tokenize.sent_tokenize(decoded_sentences)\n",
    "        generated_sentences.append(decoded_sentences_list[0])\n",
    "        \n",
    "    top_3_sentences = sort_by_similarity(full_sentence,generated_sentences)\n",
    "    \n",
    "    return top_3_sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informative-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# index = 1\n",
    "# choice_list = [\"a)\",\"b)\",\"c)\",\"d)\",\"e)\",\"f)\"]\n",
    "# True_Sentence_out = []\n",
    "\n",
    "# results = []\n",
    "# for key_sentence in sent_completion_dict:\n",
    "    \n",
    "#     false_sentences = []\n",
    "#     partial_sentences = sent_completion_dict[key_sentence]\n",
    "#     print(\"  **True Sentences (GPT-2 Generated)**\")\n",
    "#     True_Sentence = \"%s)\"%(str(index))+ str(partial_sentences)\n",
    "#     temp = {\"sentence\":partial_sentences[0],\"false_sentences\":[]}\n",
    "#     result['true'] = True_Sentence\n",
    " \n",
    "    \n",
    "    \n",
    "#     for partial_sent in partial_sentences:\n",
    "#         false_sents = generate_sentences(partial_sent,key_sentence)\n",
    "#         false_sentences.extend(false_sents)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#     temp = {\"sentence\":partial_sentences[0],\"false_sentences\":false_sentences}\n",
    "#     print(temp)\n",
    "#     results.append(temp)\n",
    "    \n",
    "    \n",
    "#     index = index+1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closing-exhaust",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:31:47.714630Z",
     "start_time": "2024-03-12T20:31:47.700931Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def store(sent_completion_dict):\n",
    "    index = 1\n",
    "\n",
    "    results = []\n",
    "    for key_sentence in sent_completion_dict:\n",
    "    \n",
    "        false_sentences = []\n",
    "        partial_sentences = sent_completion_dict[key_sentence]\n",
    "        temp = {\"sentence\":partial_sentences[0],\"false_sentences\":[]}\n",
    "    \n",
    "        for partial_sent in partial_sentences:\n",
    "            false_sents = generate_sentences(partial_sent,key_sentence)\n",
    "            false_sentences.extend(false_sents)\n",
    "\n",
    "        temp = {\"sentence\":partial_sentences[0],\"false_sentences\":false_sentences}\n",
    "        results.append(temp)\n",
    "    json_object = json.dumps(results, indent = 4)\n",
    "    return json_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "written-tablet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:32:25.328457Z",
     "start_time": "2024-03-12T20:31:52.207322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"sentence\": \"If a local government tax is legally dedicated for one or more specific purposes it is\",\n",
      "        \"false_sentences\": [\n",
      "            \"If a local government tax is legally dedicated for one or more specific purposes it is not taxable to the taxpayer.\",\n",
      "            \"If a local government tax is legally dedicated for one or more specific purposes it is not necessary to provide special exemption.\",\n",
      "            \"If a local government tax is legally dedicated for one or more specific purposes it is also authorized by statute to levy the same.\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"sentence\": \"53] Proposition 218 also requires certain taxes relating to real property (e.g., parcel taxes) be levied as\",\n",
      "        \"false_sentences\": [\n",
      "            \"53] Proposition 218 also requires certain taxes relating to real property (e.g., parcel taxes) be levied as an abatement of future expenditures by taxpayers who do not have a state-issued permit and are able, in good faith for the purpose thereof or should choose otherwise).\",\n",
      "            \"53] Proposition 218 also requires certain taxes relating to real property (e.g., parcel taxes) be levied as a special tax on all sales or other dispositions of actual and unpaid income in respect thereof, for which purpose only the proceeds from such sale shall not exceed 50 percent by taxation year [50 US.]\",\n",
      "            \"53] Proposition 218 also requires certain taxes relating to real property (e.g., parcel taxes) be levied as a result of income tax issued by the state on taxable personal, home and business properties for which receipts from sales are taxed within 100 days after issuance or in accordance with subsection 5(3)(a).\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"sentence\": \"Section 2 of Article XIII C contains the voter approval requirements for\",\n",
      "        \"false_sentences\": [\n",
      "            \"Section 2 of Article XIII C contains the voter approval requirements for public office.\",\n",
      "            \"Section 2 of Article XIII C contains the voter approval requirements for such elections.\",\n",
      "            \"Section 2 of Article XIII C contains the voter approval requirements for state elections.\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"sentence\": \"Under Proposition 218, every local government tax is either a general tax or\",\n",
      "        \"false_sentences\": []\n",
      "    },\n",
      "    {\n",
      "        \"sentence\": \"55] Proposition 218 does not allow a local tax to be\",\n",
      "        \"false_sentences\": [\n",
      "            \"55] Proposition 218 does not allow a local tax to be levied upon land use without first assessing the benefit of those lands.\",\n",
      "            \"55] Proposition 218 does not allow a local tax to be levied on individuals or estates whose property is worth more than $50,000.\",\n",
      "            \"55] Proposition 218 does not allow a local tax to be levied on the sale or disposition of certain property, but permits municipal taxation.\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"sentence\": \"62] The applicable electorate for conducting a local tax election is generally the registered voters of\",\n",
      "        \"false_sentences\": [\n",
      "            \"62] The applicable electorate for conducting a local tax election is generally the registered voters of an Election Board in every county where that city or town has been elected by referendum and, accordingly to their authority under provincial law shall ensure each voter who votes one thousand eight hundred fifty-eight (508) at this poll receives his electoral vote.\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"sentence\": \"63][64] New local government taxes require voter approval under\",\n",
      "        \"false_sentences\": [\n",
      "            \"63][64] New local government taxes require voter approval under Pennsylvania's 2008 election law [65].\",\n",
      "            \"63][64] New local government taxes require voter approval under Section 1 of state law, and they are also subject to public participation requirements.\",\n",
      "            \"63][64] New local government taxes require voter approval under federal law [65], and this is one of the main reasons that most rural voters are not satisfied with them.[66,67],[68]).\"\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "x = store(sent_completion_dict)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-patrol",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.mnightly-2021-02-02-ubuntu-1804-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:mnightly-2021-02-02-ubuntu-1804-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
