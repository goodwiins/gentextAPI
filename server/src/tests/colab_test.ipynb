{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing with GPT-2 and T5\n",
    "Install required packages first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Generator:\n",
    "    def __init__(self, model_name='gpt2-medium'):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def generate_false_statements(self, text, num_samples=3, max_length=50):\n",
    "        input_ids = self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_samples,\n",
    "            max_length=max_length,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        statements = []\n",
    "        for output in outputs:\n",
    "            generated_text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            if generated_text != text:  # Avoid exact matches\n",
    "                statements.append(generated_text)\n",
    "                \n",
    "        return statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Generator:\n",
    "    def __init__(self, model_name='t5-base'):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def generate_false_statement(self, sentence, num_candidates=3):\n",
    "        prefix = \"paraphrase: \"\n",
    "        input_text = prefix + sentence\n",
    "        \n",
    "        input_ids = self.tokenizer(input_text, return_tensors='pt').input_ids.to(self.device)\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            num_return_sequences=num_candidates,\n",
    "            do_sample=True,\n",
    "            max_length=128,\n",
    "            top_k=120,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8,\n",
    "            num_beams=4\n",
    "        )\n",
    "        \n",
    "        paraphrases = []\n",
    "        for output in outputs:\n",
    "            paraphrase = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            if paraphrase != sentence:  # Avoid exact matches\n",
    "                paraphrases.append(paraphrase)\n",
    "                \n",
    "        return paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both models\n",
    "test_text = \"The Earth revolves around the Sun.\"\n",
    "\n",
    "# Initialize generators\n",
    "gpt2_gen = GPT2Generator()\n",
    "t5_gen = T5Generator()\n",
    "\n",
    "print(\"GPT-2 Generated Statements:\")\n",
    "gpt2_results = gpt2_gen.generate_false_statements(test_text)\n",
    "for i, stmt in enumerate(gpt2_results, 1):\n",
    "    print(f\"{i}. {stmt}\")\n",
    "\n",
    "print(\"\\nT5 Generated Statements:\")\n",
    "t5_results = t5_gen.generate_false_statement(test_text)\n",
    "for i, stmt in enumerate(t5_results, 1):\n",
    "    print(f\"{i}. {stmt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process longer text\n",
    "def process_text(text, generator, sentences_per_statement=3):\n",
    "    # Split text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split()) > 5:  # Only process sentences with more than 5 words\n",
    "            if isinstance(generator, GPT2Generator):\n",
    "                variations = generator.generate_false_statements(sentence, sentences_per_statement)\n",
    "            else:\n",
    "                variations = generator.generate_false_statement(sentence, sentences_per_statement)\n",
    "            \n",
    "            results.append({\n",
    "                'original': sentence,\n",
    "                'variations': variations\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a longer text\n",
    "test_paragraph = \"\"\"\n",
    "Artificial intelligence has transformed many aspects of modern life. \n",
    "Machine learning algorithms can now recognize patterns in vast amounts of data. \n",
    "Neural networks have revolutionized image and speech recognition tasks.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Processing with GPT-2:\")\n",
    "gpt2_results = process_text(test_paragraph, gpt2_gen)\n",
    "for result in gpt2_results:\n",
    "    print(f\"\\nOriginal: {result['original']}\")\n",
    "    print(\"Variations:\")\n",
    "    for i, var in enumerate(result['variations'], 1):\n",
    "        print(f\"{i}. {var}\")\n",
    "\n",
    "print(\"\\nProcessing with T5:\")\n",
    "t5_results = process_text(test_paragraph, t5_gen)\n",
    "for result in t5_results:\n",
    "    print(f\"\\nOriginal: {result['original']}\")\n",
    "    print(\"Variations:\")\n",
    "    for i, var in enumerate(result['variations'], 1):\n",
    "        print(f\"{i}. {var}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Text_Processing_GPT2_T5.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
